{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/PhonePe/pulse.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=\"D:/GUVI_Projects/My_Projects/pulse/data/aggregated/transaction/country/india/state\"\n",
    "Agg_state_list=os.listdir(path)\n",
    "Agg_state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once created the clone of GIT-HUB repository then,\n",
    "#Required libraries for the program\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "#This is to direct the path to get the data as states\n",
    "\n",
    "path=\"D:/GUVI_Projects/My_Projects/pulse/data/aggregated/transaction/country/india/state/\"\n",
    "Agg_state_list=os.listdir(path)\n",
    "Agg_state_list\n",
    "#Agg_state_list--> to get the list of states in India\n",
    "\n",
    "#<------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------>#\n",
    "\n",
    "#This is to extract the data's to create a dataframe\n",
    "\n",
    "clm={'State':[], 'Year':[],'Quater':[],'Transaction_type':[], 'Transaction_count':[], 'Transaction_amount':[]}\n",
    "\n",
    "for i in Agg_state_list:\n",
    "    p_i=path+i+\"/\"\n",
    "    Agg_yr=os.listdir(p_i)\n",
    "    for j in Agg_yr:\n",
    "        p_j=p_i+j+\"/\"\n",
    "        Agg_yr_list=os.listdir(p_j)\n",
    "        for k in Agg_yr_list:\n",
    "            p_k=p_j+k\n",
    "            Data=open(p_k,'r')\n",
    "            D=json.load(Data)\n",
    "            for z in D['data']['transactionData']:\n",
    "              Name=z['name']\n",
    "              count=z['paymentInstruments'][0]['count']\n",
    "              amount=z['paymentInstruments'][0]['amount']\n",
    "              clm['Transaction_type'].append(Name)\n",
    "              clm['Transaction_count'].append(count)\n",
    "              clm['Transaction_amount'].append(amount)\n",
    "              clm['State'].append(i)\n",
    "              clm['Year'].append(j)\n",
    "              clm['Quater'].append(int(k.strip('.json')))\n",
    "#Succesfully created a dataframe\n",
    "Agg_Trans=pd.DataFrame(clm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_Trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:/GUVI_Projects/My_Projects/pulse/data/aggregated/insurance/country/india/state/\"\n",
    "Agg_state_list=os.listdir(path)\n",
    "Agg_state_list\n",
    "#Agg_state_list--> to get the list of states in India\n",
    "\n",
    "#<------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------>#\n",
    "\n",
    "#This is to extract the data's to create a dataframe\n",
    "\n",
    "clm={'State':[], 'Year':[],'Quater':[],'Insurance_type':[], 'Insurance_count':[], 'Insurance_amount':[]}\n",
    "\n",
    "for i in Agg_state_list:\n",
    "    p_i=path+i+\"/\"\n",
    "    Agg_yr=os.listdir(p_i)\n",
    "    for j in Agg_yr:\n",
    "        p_j=p_i+j+\"/\"\n",
    "        Agg_yr_list=os.listdir(p_j)\n",
    "        for k in Agg_yr_list:\n",
    "            p_k=p_j+k\n",
    "            Data=open(p_k,'r')\n",
    "            D=json.load(Data)\n",
    "            for z in D['data']['transactionData']:\n",
    "              Name=z['name']\n",
    "              \n",
    "            \n",
    "              count=z['paymentInstruments'][0]['count']\n",
    "              amount=z['paymentInstruments'][0]['amount']\n",
    "              \n",
    "              clm['Insurance_type'].append(\"TOTAL\")\n",
    "              clm['Insurance_count'].append(count)\n",
    "              clm['Insurance_amount'].append(amount)\n",
    "              clm['State'].append(i)\n",
    "              clm['Year'].append(j)\n",
    "              clm['Quater'].append(int(k.strip('.json')))\n",
    "#Succesfully created a dataframe\n",
    "Agg_Insurance=pd.DataFrame(clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/GUVI_Projects/My_Projects/pulse/data/aggregated/user/country/india/state/\"\n",
    "Agg_state_list=os.listdir(path)\n",
    "Agg_state_list\n",
    "\n",
    "\n",
    "\n",
    "# Define the structure of the DataFrame for user data\n",
    "clm = {'State': [], 'Year': [], 'Quarter': [], 'Brand': [], 'User_count': [], 'Percentage': []}\n",
    "\n",
    "for state in Agg_state_list:\n",
    "    state_path = path + state + \"/\"\n",
    "    Agg_yr = os.listdir(state_path)\n",
    "    for year in Agg_yr:\n",
    "        year_path = state_path + year + \"/\"\n",
    "        Agg_qtr_list = os.listdir(year_path)\n",
    "        for qtr_file in Agg_qtr_list:\n",
    "            qtr_path = year_path + qtr_file\n",
    "            with open(qtr_path, 'r') as Data:\n",
    "                D = json.load(Data)\n",
    "                if 'data' in D and 'usersByDevice' in D['data']:\n",
    "                    if D['data']['usersByDevice']:\n",
    "                        users_by_device = D['data']['usersByDevice']\n",
    "                        for z in users_by_device:\n",
    "                            brand = z.get('brand', \"Unknown\")\n",
    "                            count = z.get('count', 0)\n",
    "                            percentage = z.get('percentage', 0.0)\n",
    "                            clm['Brand'].append(brand)\n",
    "                            clm['User_count'].append(count)\n",
    "                            clm['Percentage'].append(percentage)\n",
    "                            clm['State'].append(state)\n",
    "                            clm['Year'].append(year)\n",
    "                            clm['Quarter'].append(int(qtr_file.strip('.json')))\n",
    "                    else:\n",
    "                        print(f\"Data present but empty 'usersByDevice' in file: {qtr_path}\")\n",
    "                else:\n",
    "                    print(f\"No 'usersByDevice' key found in file: {qtr_path}\")\n",
    "# Create a DataFrame from the collected data\n",
    "Agg_User = pd.DataFrame(clm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_User \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "def insert_dataframes_into_sql(engine_url, dataframe_dict):\n",
    "    # Create a database engine\n",
    "    engine = create_engine(engine_url)\n",
    "\n",
    "    # Iterate through the dictionary and insert each DataFrame into its respective table\n",
    "    for table_name, dataframe in dataframe_dict.items():\n",
    "        dataframe.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "    \n",
    "    # Dispose the engine to close the connection properly\n",
    "    engine.dispose()\n",
    "\n",
    "# Example usage\n",
    "engine_url = \"postgresql://postgres:admin@localhost:5432/phone_pe1\"\n",
    "dataframes = {\n",
    "        \"Agg_Insurance\": Agg_Insurance,  # Assuming Agg_Trans is similar to your channel_data\n",
    "        \"Agg_Trans\": Agg_Trans,  # Assuming Agg_Insurance is similar to your video_data\n",
    "        \"Agg_User\": Agg_User  # Assuming Agg_User is similar to your comments_data\n",
    "        \n",
    "    }\n",
    "\n",
    "insert_dataframes_into_sql(engine_url, dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the directory containing state subdirectories with JSON files\n",
    "path = \"D:/GUVI_Projects/My_Projects/pulse/data/map/insurance/country/india/state/\"\n",
    "map_state_list = os.listdir(path)\n",
    "print(\"Directories in path:\", map_state_list)\n",
    "\n",
    "# Data structure to hold extracted values\n",
    "clm = {'lat': [], 'lng': [], 'metric': [], 'label': [], 'Year': []}\n",
    "\n",
    "# Process each state directory\n",
    "for state in map_state_list:\n",
    "    state_path = os.path.join(path, state)\n",
    "    if os.path.isdir(state_path):  # Check if it is a directory before processing\n",
    "        Agg_yr = os.listdir(state_path)\n",
    "        for year in Agg_yr:\n",
    "            year_path = os.path.join(state_path, year)\n",
    "            if os.path.isdir(year_path):  # Check if it is a directory before processing\n",
    "                Agg_qtr_list = os.listdir(year_path)\n",
    "                for qtr_file in Agg_qtr_list:\n",
    "                    if qtr_file.endswith('.json'):  # Only process JSON files\n",
    "                        qtr_path = os.path.join(year_path, qtr_file)\n",
    "                        print(f\"Processing file: {qtr_path}\")\n",
    "                        with open(qtr_path, 'r') as Data:\n",
    "                            try:\n",
    "                                D = json.load(Data)\n",
    "                                if D.get('success'):\n",
    "                                    columns = D['data']['data']['columns']\n",
    "                                    for entry in D['data']['data']['data']:\n",
    "                                        clm['lat'].append(entry[columns.index('lat')])\n",
    "                                        clm['lng'].append(entry[columns.index('lng')])\n",
    "                                        clm['metric'].append(entry[columns.index('metric')])\n",
    "                                        clm['label'].append(entry[columns.index('label')])\n",
    "                                        clm['Year'].append(year)\n",
    "                                        print(f\"Data added for {qtr_file}: {entry}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping file due to unsuccessful load: {qtr_path}\")\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON in {qtr_file}: {e}\")\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Key error in {qtr_file}: {e}\")\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "map_Insurance = pd.DataFrame(clm)\n",
    "\n",
    "# Display the first few rows of the DataFrame if not empty\n",
    "if not map_Insurance.empty:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(map_Insurance.head())\n",
    "else:\n",
    "    print(\"The DataFrame is empty. Check the JSON file content and structure.\")\n",
    "\n",
    "# Summarize data by 'Year'\n",
    "if not map_Insurance.empty:\n",
    "    yearly_summary = map_Insurance.groupby('Year').agg({\n",
    "        'metric': ['mean', 'sum', 'count']\n",
    "    })\n",
    "    print(\"Yearly Summary:\")\n",
    "    print(yearly_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the directory containing state subdirectories with JSON files\n",
    "path = \"D:/GUVI_Projects/My_Projects/pulse/data/map/transaction/hover/country/india/state/\"\n",
    "map_state_list = os.listdir(path)\n",
    "print(\"Directories in path:\", map_state_list)\n",
    "\n",
    "# Data structure to hold extracted values\n",
    "clm = {'name': [], 'type': [], 'count': [], 'amount': [], 'Year': []}\n",
    "\n",
    "# Process each state directory\n",
    "for state in map_state_list:\n",
    "    state_path = os.path.join(path, state)\n",
    "    if os.path.isdir(state_path):  # Check if it is a directory before processing\n",
    "        map_yr = os.listdir(state_path)\n",
    "        for year in map_yr:\n",
    "            year_path = os.path.join(state_path, year)\n",
    "            if os.path.isdir(year_path):  # Check if it is a directory before processing\n",
    "                map_qtr_list = os.listdir(year_path)\n",
    "                for qtr_file in map_qtr_list:\n",
    "                    if qtr_file.endswith('.json'):  # Only process JSON files\n",
    "                        qtr_path = os.path.join(year_path, qtr_file)\n",
    "                        print(f\"Processing file: {qtr_path}\")\n",
    "                        with open(qtr_path, 'r') as Data:\n",
    "                            try:\n",
    "                                D = json.load(Data)\n",
    "                                if D.get('success'):\n",
    "                                    hoverDataList = D['data']['hoverDataList']\n",
    "                                    for entry in hoverDataList:\n",
    "                                        name = entry['name']\n",
    "                                        for metric in entry['metric']:\n",
    "                                            clm['name'].append(name)\n",
    "                                            clm['type'].append(metric['type'])\n",
    "                                            clm['count'].append(metric['count'])\n",
    "                                            clm['amount'].append(metric['amount'])\n",
    "                                            clm['Year'].append(year)\n",
    "                                            print(f\"Data added for {qtr_file}: {entry}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping file due to unsuccessful load: {qtr_path}\")\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON in {qtr_file}: {e}\")\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Key error in {qtr_file}: {e}\")\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "map_transaction = pd.DataFrame(clm)\n",
    "\n",
    "# Display the first few rows of the DataFrame if not empty\n",
    "if not map_transaction.empty:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(map_transaction.head())\n",
    "else:\n",
    "    print(\"The DataFrame is empty. Check the JSON file content and structure.\")\n",
    "\n",
    "# Summarize data by 'Year'\n",
    "if not map_transaction.empty:\n",
    "    yearly_summary = map_transaction.groupby('Year').agg({\n",
    "        'count': ['sum', 'mean', 'count'],\n",
    "        'amount': ['sum', 'mean']\n",
    "    })\n",
    "    print(\"Yearly Summary:\")\n",
    "    print(yearly_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the directory containing state subdirectories with JSON files\n",
    "path = \"D:/GUVI_Projects/My_Projects/pulse/data/map/user/hover/country/india/state/\"\n",
    "map_state_list = os.listdir(path)\n",
    "\n",
    "# Data structure to hold extracted values\n",
    "clm = {'State': [], 'RegisteredUsers': [], 'AppOpens': [], 'Year': []}\n",
    "\n",
    "# Process each state directory\n",
    "for state in map_state_list:\n",
    "    state_path = os.path.join(path, state)\n",
    "    if os.path.isdir(state_path):  # Check if it is a directory before processing\n",
    "        map_yr = os.listdir(state_path)\n",
    "        for year in map_yr:\n",
    "            year_path = os.path.join(state_path, year)\n",
    "            if os.path.isdir(year_path):  # Check if it is a directory before processing\n",
    "                year_qtr_list = os.listdir(year_path)\n",
    "                for qtr_file in year_qtr_list:\n",
    "                    qtr_path = os.path.join(year_path, qtr_file)\n",
    "                    if qtr_file.endswith('.json'):  # Only process JSON files\n",
    "                        print(f\"Processing file: {qtr_path}\")\n",
    "                        with open(qtr_path, 'r') as Data:\n",
    "                            try:\n",
    "                                D = json.load(Data)\n",
    "                                if D.get('success'):\n",
    "                                    hoverData = D['data']['hoverData']\n",
    "                                    for state, data in hoverData.items():\n",
    "                                        clm['State'].append(state)\n",
    "                                        clm['RegisteredUsers'].append(data['registeredUsers'])\n",
    "                                        clm['AppOpens'].append(data['appOpens'])\n",
    "                                        clm['Year'].append(year)\n",
    "                                        print(f\"Data added for {state} in {year}: {data}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping file due to unsuccessful load: {qtr_path}\")\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON in {qtr_path}: {e}\")\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "map_User = pd.DataFrame(clm)\n",
    "\n",
    "# Display the first few rows of the DataFrame if not empty\n",
    "if not map_User.empty:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(map_User.head())\n",
    "else:\n",
    "    print(\"The DataFrame is empty. Check the JSON file content and structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "def insert_dataframes_into_sql(engine_url, dataframe_dict):\n",
    "    # Create a database engine\n",
    "    engine = create_engine(engine_url)\n",
    "\n",
    "    # Iterate through the dictionary and insert each DataFrame into its respective table\n",
    "    for table_name, dataframe in dataframe_dict.items():\n",
    "        # Insert data into the SQL table using the 'multi' method to improve performance on larger datasets\n",
    "        dataframe.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "    \n",
    "    # Dispose the engine to close the connection properly\n",
    "    engine.dispose()\n",
    "\n",
    "# Example usage\n",
    "engine_url = \"postgresql://postgres:admin@localhost:5432/phone_pe1\"\n",
    "dataframes = {\n",
    "    \"map_transaction\": map_transaction,  # Assuming this DataFrame is prepared earlier in your code\n",
    "    \"map_User\": map_User,  # Assuming this DataFrame is prepared earlier in your code\n",
    "    \"map_Insurance\": map_Insurance  # Assuming this DataFrame is prepared earlier in your code\n",
    "}\n",
    "\n",
    "insert_dataframes_into_sql(engine_url, dataframes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
